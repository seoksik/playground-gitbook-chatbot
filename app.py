import streamlit as st

# ìŠ¤íŠ¸ë¦¼ë¦¿ í˜ì´ì§€ êµ¬ì„± ì„¤ì • (ë°˜ë“œì‹œ ë‹¤ë¥¸ st ëª…ë ¹ ì „ì— í˜¸ì¶œí•´ì•¼ í•¨)
st.set_page_config(page_title="Gitbook Q&A Chatbot", layout="wide", initial_sidebar_state="expanded")

import os
import json
import datetime
import random
from dotenv import load_dotenv

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores.supabase import SupabaseVectorStore
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from supabase.client import Client, create_client

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_ANON_KEY = os.getenv("SUPABASE_ANON_KEY")
TARGET_GITBOOK_NAME = os.getenv("TARGET_GITBOOK_NAME", "í•´ë‹¹ Gitbook")
CHAT_HISTORY_FILE = os.getenv("CHAT_HISTORY_FILE", "chat_history.json")

# ì¶”ì²œ ì§ˆë¬¸ ëª©ë¡ - ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ì— ë§ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•„ìš”
DEFAULT_SUGGESTED_QUESTIONS = [
    "Gitbookì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
    "FETA í”„ë¡œì íŠ¸ì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "API ë¬¸ì„œëŠ” ì–´ë””ì„œ í™•ì¸í•  ìˆ˜ ìˆë‚˜ìš”?",
    "ê°œë°œì ê°€ì´ë“œëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
    "ì„¤ì¹˜ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
    "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€"
]

# ë²¡í„° DB ê¸°ë°˜ ì´ˆê¸° ì¶”ì²œ ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜
def generate_initial_questions(vector_store, supabase_client, llm, num_questions=4):
    try:
        # ë²¡í„° DBì—ì„œ ëŒ€í‘œì ì¸ ë¬¸ì„œ ê²€ìƒ‰ (ì„ë² ë”© ì—†ì´ ìµœê·¼ ì¶”ê°€ëœ ë¬¸ì„œë“¤)
        results = supabase_client.from_("documents").select("content, metadata").limit(5).execute()
        
        if not results.data or len(results.data) == 0:
            return random.sample(DEFAULT_SUGGESTED_QUESTIONS, min(num_questions, len(DEFAULT_SUGGESTED_QUESTIONS)))
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
        doc_contents = []
        for doc in results.data:
            content = doc.get('content', '')
            if content and len(content) > 50:  # ì¶©ë¶„íˆ ì˜ë¯¸ ìˆëŠ” ì½˜í…ì¸ ë§Œ í¬í•¨
                doc_contents.append(content[:500])  # ê° ë¬¸ì„œì˜ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
        
        if not doc_contents:
            return random.sample(DEFAULT_SUGGESTED_QUESTIONS, min(num_questions, len(DEFAULT_SUGGESTED_QUESTIONS)))
        
        # ë¬¸ì„œ ë‚´ìš©ì„ í†µí•©í•˜ì—¬ LLMì— ì§ˆë¬¸ ìƒì„± ìš”ì²­
        combined_content = "\n\n".join(doc_contents[:3])  # ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ìµœëŒ€ 3ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
        
        prompt = f"""
        ë‹¤ìŒì€ ë¬¸ì„œ ì‹œìŠ¤í…œì— ì €ì¥ëœ ì½˜í…ì¸ ì˜ ì¼ë¶€ì…ë‹ˆë‹¤:
        {combined_content}

        ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¼ ë§Œí•œ ì˜ë¯¸ ìˆëŠ” ì§ˆë¬¸ {num_questions}ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì´ ì§ˆë¬¸ë“¤ì€ ë¬¸ì„œ ì‹œìŠ¤í…œì´ ì‹¤ì œë¡œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        ì§§ê³  ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        JSON í˜•ì‹ ì—†ì´ ì§ˆë¬¸ë§Œ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”.
        """
        
        # LLMìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
        response = llm.invoke(prompt)
        questions = response.content.strip().split('\n')
        
        # ë¹ˆ ì¤„ ì œê±°í•˜ê³  ì•ë’¤ ê³µë°± ì œê±°
        questions = [q.strip() for q in questions if q.strip()]
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        unique_questions = []
        for q in questions:
            if q not in unique_questions:
                unique_questions.append(q)
                if len(unique_questions) >= num_questions:
                    break
        
        # ì§ˆë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ë³´ì¶©
        if len(unique_questions) < num_questions:
            remaining = num_questions - len(unique_questions)
            default_samples = random.sample(DEFAULT_SUGGESTED_QUESTIONS, min(remaining, len(DEFAULT_SUGGESTED_QUESTIONS)))
            unique_questions.extend(default_samples)
        
        return unique_questions[:num_questions]
    except Exception as e:
        print(f"ì´ˆê¸° ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return random.sample(DEFAULT_SUGGESTED_QUESTIONS, min(num_questions, len(DEFAULT_SUGGESTED_QUESTIONS)))

# ë²¡í„° DBì—ì„œ ì„ë² ë”© ê²€ìƒ‰ì„ í†µí•œ ê³ ê¸‰ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜
def generate_advanced_initial_questions(vector_store, embeddings, supabase_client, llm, num_questions=4):
    try:
        # ì£¼ìš” ì£¼ì œì–´ ë¦¬ìŠ¤íŠ¸ - ë¬¸ì„œì— ì í•©í•œ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ
        topic_keywords = [
            "ê°œìš”", "ì„¤ì¹˜", "ì‹œì‘í•˜ê¸°", "ê¸°ëŠ¥", "ì‚¬ìš©ë²•", "FAQ", 
            "ì£¼ìš” ê¸°ëŠ¥", "ê°€ì´ë“œ", "íŠœí† ë¦¬ì–¼", "API"
        ]
        
        # ê° ì£¼ì œì–´ì— ëŒ€í•œ ì„ë² ë”© ìƒì„± í›„ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        all_docs = []
        
        # ì£¼ì œì–´ ê¸°ë°˜ ê²€ìƒ‰
        for keyword in topic_keywords[:5]:  # ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            try:
                # í‚¤ì›Œë“œ ì„ë² ë”© ìƒì„±
                query_embedding = embeddings.embed_query(keyword)
                
                # ì¿¼ë¦¬ ë²¡í„°ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
                function_params = {
                    "query_embedding": query_embedding,
                    "match_threshold": 0.5,
                    "match_count": 2  # ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 2ê°œ ë¬¸ì„œ
                }
                
                results = supabase_client.rpc(
                    "match_documents", function_params
                ).execute()
                
                # ê²°ê³¼ ì¶”ê°€
                if results.data:
                    for doc in results.data:
                        if doc.get('content') and len(doc.get('content')) > 100:
                            all_docs.append(doc)
            except Exception as e:
                print(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        # ì¶”ê°€ ê²€ìƒ‰: ìµœì‹  ë¬¸ì„œë„ í¬í•¨
        try:
            recent_docs = supabase_client.from_("documents").select("content, metadata").order("id", desc=True).limit(3).execute()
            if recent_docs.data:
                all_docs.extend(recent_docs.data)
        except Exception as e:
            print(f"ìµœì‹  ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²€ìƒ‰ ë°©ë²• ì‚¬ìš©
        if not all_docs:
            return generate_initial_questions(vector_store, supabase_client, llm, num_questions)
        
        # ì¤‘ë³µ ì œê±° ë° ë‚´ìš© ì¶”ì¶œ
        unique_contents = []
        seen_contents = set()
        
        for doc in all_docs:
            content = doc.get('content', '')
            # ê°™ì€ ë‚´ìš©ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸ (ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì•ë¶€ë¶„ë§Œ ì²´í¬)
            content_start = content[:100] if len(content) > 100 else content
            
            if content and content_start not in seen_contents:
                seen_contents.add(content_start)
                # ê¸´ ë¬¸ì„œëŠ” ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
                if len(content) > 800:
                    content = content[:800] + "..."
                unique_contents.append(content)
        
        # ìµœì ì˜ ê²°ê³¼ë¥¼ ìœ„í•´ ë¬¸ì„œ 3~5ê°œë§Œ ì‚¬ìš©
        selected_contents = unique_contents[:5]
        
        if not selected_contents:
            return generate_initial_questions(vector_store, supabase_client, llm, num_questions)
        
        # ë¬¸ì„œ ë‚´ìš© í†µí•©
        combined_content = "\n\n---\n\n".join(selected_contents)
        
        # LLM í”„ë¡¬í”„íŠ¸ ì‘ì„±
        prompt = f"""
        ë‹¤ìŒì€ ë¬¸ì„œ ì‹œìŠ¤í…œì— ì €ì¥ëœ ì‹¤ì œ ì½˜í…ì¸ ì˜ ì¼ë¶€ì…ë‹ˆë‹¤:
        
        {combined_content}

        ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ì •í™•íˆ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ì§ˆë¬¸ {num_questions}ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
        1. ì‹œìŠ¤í…œì´ ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í™•ì‹¤íˆ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë§Œ ìƒì„±í•˜ì„¸ìš”.
        2. ì§ˆë¬¸ì€ êµ¬ì²´ì ì´ê³  ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.
        3. ì§ˆë¬¸ì€ ë¬¸ì„œ ë‚´ìš©ì— í¬í•¨ëœ ì£¼ìš” ê°œë…, ê¸°ëŠ¥, ë°©ë²• ë“±ì„ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤.
        4. ë‹¤ì–‘í•œ ì£¼ì œë¥¼ ë‹¤ë£¨ë„ë¡ ì§ˆë¬¸ì„ ë¶„ì‚°ì‹œí‚¤ì„¸ìš”.
        
        JSON í˜•ì‹ ì—†ì´ ì§ˆë¬¸ë§Œ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”.
        """
        
        # LLMìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
        response = llm.invoke(prompt)
        questions = response.content.strip().split('\n')
        
        # ë¹ˆ ì¤„ ì œê±°í•˜ê³  ì•ë’¤ ê³µë°± ì œê±°
        questions = [q.strip() for q in questions if q.strip()]
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        unique_questions = []
        for q in questions:
            if q not in unique_questions:
                unique_questions.append(q)
                if len(unique_questions) >= num_questions:
                    break
        
        # ì§ˆë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ë³´ì¶©
        if len(unique_questions) < num_questions:
            remaining = num_questions - len(unique_questions)
            default_samples = random.sample(DEFAULT_SUGGESTED_QUESTIONS, min(remaining, len(DEFAULT_SUGGESTED_QUESTIONS)))
            unique_questions.extend(default_samples)
        
        return unique_questions[:num_questions]
    except Exception as e:
        print(f"ê³ ê¸‰ ì´ˆê¸° ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ í´ë°±
        return generate_initial_questions(vector_store, supabase_client, llm, num_questions)

# ë¬¸ë§¥ë³„ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜
def generate_context_questions(last_answer, llm):
    try:
        if not last_answer or len(last_answer) < 20:
            return []
        
        prompt = f"""
        ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤:
        {last_answer[:500]}...

        ìœ„ ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ë‹¤ìŒìœ¼ë¡œ ë¬¼ì–´ë³¼ ë§Œí•œ ê´€ë ¨ ì§ˆë¬¸ 3ê°€ì§€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì§§ê³  ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. 
        JSON í˜•ì‹ ì—†ì´ ì§ˆë¬¸ë§Œ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”.
        """
        
        # LLMìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
        response = llm.invoke(prompt)
        questions = response.content.strip().split('\n')
        
        # ë¹ˆ ì¤„ ì œê±°í•˜ê³  ì•ë’¤ ê³µë°± ì œê±°
        questions = [q.strip() for q in questions if q.strip()]
        
        # ìµœëŒ€ 3ê°œ ë°˜í™˜
        return questions[:3]
    except Exception as e:
        print(f"ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return []

# ì±„íŒ… ë‚´ì—­ ì €ì¥ í•¨ìˆ˜
def save_chat_history():
    chat_data = {
        "chat_history": st.session_state.chat_history,
    }
    
    # ê° ëŒ€í™” ë‚´ì—­ë„ ì €ì¥
    for chat_name, chat_id in st.session_state.chat_history:
        chat_key = f"chat_{chat_id}"
        if chat_key in st.session_state:
            chat_data[chat_key] = st.session_state[chat_key]
    
    try:
        with open(CHAT_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(chat_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.warning(f"ì±„íŒ… ë‚´ì—­ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì±„íŒ… ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_chat_history():
    try:
        if os.path.exists(CHAT_HISTORY_FILE):
            with open(CHAT_HISTORY_FILE, 'r', encoding='utf-8') as f:
                chat_data = json.load(f)
                
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
            if "chat_history" in chat_data:
                st.session_state.chat_history = chat_data["chat_history"]
            
            # ê° ëŒ€í™” ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸°
            for key, value in chat_data.items():
                if key.startswith("chat_") and key != "chat_history":
                    st.session_state[key] = value
    except Exception as e:
        st.warning(f"ì±„íŒ… ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì´ˆê¸°í™”
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []

# í™˜ê²½ ë³€ìˆ˜ ìœ íš¨ì„± ê²€ì‚¬
if not OPENAI_API_KEY or not SUPABASE_URL or not SUPABASE_ANON_KEY:
    missing_vars = []
    if not OPENAI_API_KEY:
        missing_vars.append("OPENAI_API_KEY")
    if not SUPABASE_URL:
        missing_vars.append("SUPABASE_URL")
    if not SUPABASE_ANON_KEY:
        missing_vars.append("SUPABASE_ANON_KEY")
    
    st.error(f"í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing_vars)}")
    st.info("create_env.py ìŠ¤í¬ë¦½íŠ¸ë¡œ ìƒì„±ëœ .env íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ í•„ìš”í•œ ê°’ì„ ì±„ì›Œì£¼ì„¸ìš”.")
    st.stop()

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìºì‹±)
@st.cache_resource
def init_supabase_client():
    try:
        return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)
    except Exception as e:
        st.error(f"Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

# ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìºì‹±)
@st.cache_resource
def init_chat_model():
    return ChatOpenAI(
        temperature=0.1, 
        model_name='gpt-3.5-turbo', 
        openai_api_key=OPENAI_API_KEY
    )

supabase_client = init_supabase_client()
if not supabase_client:
    st.stop()

# LLM ëª¨ë¸ ì´ˆê¸°í™”
llm = init_chat_model()

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ)
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
    load_chat_history()

# ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ì„¸ì…˜ ìƒíƒœ ì‚¬ìš©)
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

# Langchain êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìºì‹±)
@st.cache_resource
def init_langchain_components(_supabase_client, _memory): 
    try:
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        
        # Supabase Vector Store ì´ˆê¸°í™”
        vector_store = SupabaseVectorStore(
            client=_supabase_client,
            embedding=embeddings,
            table_name="documents",
            query_name="match_documents"
        )
        
        llm = ChatOpenAI(
            temperature=0.1, 
            model_name='gpt-3.5-turbo', 
            openai_api_key=OPENAI_API_KEY
        )
        
        # ê²€ìƒ‰ê¸° ì„¤ì •
        retriever = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={
                'k': 5, 
                'score_threshold': 0.5, 
                'filter': {} 
            } 
        )
        
        # ConversationalRetrievalChain ì‚¬ìš© (ëŒ€í™” ê¸°ì–µ ê¸°ëŠ¥ í¬í•¨)
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=_memory,
            return_source_documents=True,
            return_generated_question=True,
        )
            
        return qa_chain, vector_store, llm, embeddings
    except Exception as e:
        st.error(f"Langchain êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None, None, None, None

qa_result = init_langchain_components(supabase_client, st.session_state.memory)
if not qa_result or qa_result[0] is None:
    st.stop()

qa_chain, vector_store, qa_llm, embeddings = qa_result

# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! Gitbook ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]

# --- ê°„ë‹¨í•œ ë³´ì™„ ì˜µì…˜: ë²¡í„° DB ì—†ì´ë„ ì‘ë™í•  ìˆ˜ ìˆë„ë¡ ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš© --- #
def get_default_questions(num_questions=4):
    """ë²¡í„° DB ì ‘ê·¼ì— ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ì§ˆë¬¸ ë°˜í™˜"""
    return random.sample(DEFAULT_SUGGESTED_QUESTIONS, min(num_questions, len(DEFAULT_SUGGESTED_QUESTIONS)))

# ëŒ€ì²´ ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜ - ìˆ˜ì •ëœ ë²„ì „
def generate_alternative_questions(supabase_client, llm, num_questions=4):
    """ë°ì´í„°ë² ì´ìŠ¤ ì§ì ‘ ì¿¼ë¦¬ë¥¼ í†µí•œ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±"""
    try:
        # ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¬¸ì„œ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
        results = supabase_client.from_("documents").select("content").limit(5).execute()
        
        if not results.data or len(results.data) == 0:
            print("ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ë°ì´í„°ë² ì´ìŠ¤ ì ‘ê·¼ ì‹¤íŒ¨")
            return get_default_questions(num_questions)
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
        doc_contents = []
        for doc in results.data:
            content = doc.get('content', '')
            if content and len(content) > 50:
                doc_contents.append(content[:500]) 
        
        if not doc_contents:
            print("ìœ íš¨í•œ ë¬¸ì„œ ì½˜í…ì¸  ì—†ìŒ")
            return get_default_questions(num_questions)
        
        # ë¬¸ì„œ ë‚´ìš© ê²°í•©
        combined_content = "\n\n".join(doc_contents[:3])
        
        prompt = f"""
        ë‹¤ìŒì€ ë¬¸ì„œ ì‹œìŠ¤í…œì— ì €ì¥ëœ ì½˜í…ì¸ ì˜ ì¼ë¶€ì…ë‹ˆë‹¤:
        {combined_content}

        ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¼ ë§Œí•œ ì˜ë¯¸ ìˆëŠ” ì§ˆë¬¸ {num_questions}ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì´ ì§ˆë¬¸ë“¤ì€ ë¬¸ì„œ ì‹œìŠ¤í…œì´ ì‹¤ì œë¡œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        ì§§ê³  ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        JSON í˜•ì‹ ì—†ì´ ì§ˆë¬¸ë§Œ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”.
        """
        
        response = llm.invoke(prompt)
        questions = response.content.strip().split('\n')
        
        # ë¹ˆ ì¤„ ì œê±°í•˜ê³  ì•ë’¤ ê³µë°± ì œê±°
        questions = [q.strip() for q in questions if q.strip()]
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        unique_questions = []
        for q in questions:
            if q not in unique_questions:
                unique_questions.append(q)
                if len(unique_questions) >= num_questions:
                    break
        
        # ì§ˆë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ë³´ì¶©
        if len(unique_questions) < num_questions:
            remaining = num_questions - len(unique_questions)
            default_samples = get_default_questions(remaining)
            unique_questions.extend(default_samples)
        
        return unique_questions[:num_questions]
    except Exception as e:
        print(f"ëŒ€ì²´ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return get_default_questions(num_questions)

# ì¶”ì²œ ì§ˆë¬¸ ì´ˆê¸°í™” - ë²¡í„° DB ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
if "suggested_questions" not in st.session_state:
    try:
        # ë²¡í„° DB ê¸°ë°˜ ê³ ê¸‰ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì‹œë„
        initial_questions = generate_advanced_initial_questions(vector_store, embeddings, supabase_client, qa_llm, num_questions=4)
    except Exception as e:
        print(f"ê³ ê¸‰ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        try:
            # ëŒ€ì²´ ë°©ì‹ ì‹œë„
            initial_questions = generate_alternative_questions(supabase_client, qa_llm, num_questions=4)
        except Exception as backup_e:
            print(f"ëŒ€ì²´ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±ë„ ì‹¤íŒ¨: {backup_e}")
            # ë§ˆì§€ë§‰ ëŒ€ì•ˆìœ¼ë¡œ ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©
            initial_questions = get_default_questions(4)
    
    st.session_state.suggested_questions = initial_questions

# ì¶”ì²œ ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜
def handle_suggested_question(question):
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ì±„íŒ…ì°½ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": question})
    
    # ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response_content = ""
        
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... ğŸ¤”"):
            try:
                # Langchain QA ì‹¤í–‰
                response = qa_chain.invoke({"question": question})
                
                # ì‘ë‹µ ì¶”ì¶œ
                answer = response.get("answer", "")
                source_documents = response.get("source_documents", [])
                    
                if not answer:
                    answer = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ëª…í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

                full_response_content = answer

                if source_documents:
                    full_response_content += "\n\n---\n**ì°¸ê³  ë¬¸ì„œ:**\n"
                    # ì¤‘ë³µëœ source URLì„ ì œê±°í•˜ê¸° ìœ„í•œ set
                    unique_sources = set()
                    for doc in source_documents:
                        source_url = doc.metadata.get('source', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')
                        if source_url not in unique_sources and source_url != 'ì¶œì²˜ ì •ë³´ ì—†ìŒ':
                            # URLì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì œëª©ì²˜ëŸ¼ ì‚¬ìš©
                            link_title = source_url.split('/')[-1] or source_url.split('/')[-2] or "ë¬¸ì„œ"
                            link_title = link_title.replace('-', ' ').title() # ê°€ë…ì„± í–¥ìƒ
                            full_response_content += f"- [{link_title}]({source_url})\n"
                            unique_sources.add(source_url)
                
                message_placeholder.markdown(full_response_content)
                
                # ë§¥ë½ì— ë§ëŠ” ìƒˆë¡œìš´ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±
                context_questions = generate_context_questions(answer, qa_llm)
                if context_questions:
                    st.session_state.suggested_questions = context_questions
                else:
                    # ìƒˆë¡œìš´ ê¸°ë³¸ ì§ˆë¬¸ í‘œì‹œ
                    st.session_state.suggested_questions = random.sample(
                        DEFAULT_SUGGESTED_QUESTIONS, 
                        min(3, len(DEFAULT_SUGGESTED_QUESTIONS))
                    )

            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                full_response_content = "ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                message_placeholder.markdown(full_response_content)
                
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì¶”ì²œ ì§ˆë¬¸ í‘œì‹œ
                st.session_state.suggested_questions = random.sample(
                    DEFAULT_SUGGESTED_QUESTIONS, 
                    min(3, len(DEFAULT_SUGGESTED_QUESTIONS))
                )

        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": full_response_content})
        
        # ëŒ€í™” ìë™ ì €ì¥
        # í˜„ì¬ ëŒ€í™” ì´ë¦„ ì €ì¥
        if len(st.session_state.messages) == 3:  # ì²« ë²ˆì§¸ ì§ˆë¬¸ í›„ ì œëª© ìƒì„±
            first_user_msg = question
            chat_title = first_user_msg[:15] + ("..." if len(first_user_msg) > 15 else "")
            st.session_state["current_time_str"] = chat_title
    
    # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
    st.rerun()

# --- Streamlit UI ---
st.title("ğŸ“š Gitbook Q&A Chatbot")
st.caption(f"'{TARGET_GITBOOK_NAME}' ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì„œë¹„ìŠ¤") 

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("ì±—ë´‡ ì„¤ì •")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì„¹ì…˜ ì¶”ê°€
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬")

# ì €ì¥ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
history_cols = st.sidebar.columns([4, 1])
for i, (chat_name, chat_id) in enumerate(st.session_state.chat_history):
    # ëŒ€í™” ì„ íƒ
    if history_cols[0].button(f"{chat_name}", key=f"history_{i}", use_container_width=True):
        # ì„ íƒí•œ ëŒ€í™” ë‚´ìš© ë¶ˆëŸ¬ì˜¤ê¸°
        st.session_state.messages = st.session_state[f"chat_{chat_id}"].copy()
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (í•´ë‹¹ ëŒ€í™”ì— ë§ê²Œ)
        st.session_state.memory.clear()
        # ë©”ëª¨ë¦¬ ì¬êµ¬ì„± (ëŒ€í™” ë‚´ìš© ê¸°ë°˜)
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                user_msg = msg["content"]
            elif msg["role"] == "assistant" and user_msg:
                st.session_state.memory.chat_memory.add_user_message(user_msg)
                st.session_state.memory.chat_memory.add_ai_message(msg["content"])
                user_msg = None
        st.rerun()
    
    # ëŒ€í™” ì‚­ì œ ë²„íŠ¼
    if history_cols[1].button("ğŸ—‘ï¸", key=f"delete_{i}", help="ì´ ëŒ€í™” ì‚­ì œí•˜ê¸°"):
        # ëŒ€í™” ì‚­ì œ í™•ì¸
        chat_key = f"chat_{chat_id}"
        if chat_key in st.session_state:
            del st.session_state[chat_key]
        
        # íˆìŠ¤í† ë¦¬ì—ì„œ ì œê±°
        st.session_state.chat_history.pop(i)
        save_chat_history()
        st.rerun()

# ìƒˆ ëŒ€í™” ì‹œì‘ ë²„íŠ¼
if st.sidebar.button("â• ìƒˆ ëŒ€í™” ì‹œì‘", use_container_width=True):
    # í˜„ì¬ ëŒ€í™”ê°€ ìˆìœ¼ë©´ ì €ì¥
    if "messages" in st.session_state and len(st.session_state.messages) > 1:
        # ìƒˆ ëŒ€í™” ID ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
        timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        chat_id = f"chat_{timestamp}"
        
        # ëŒ€í™” ì œëª© ìƒì„±
        current_title = st.session_state.get("current_time_str", "ìƒˆ ëŒ€í™”")
        
        # í˜„ì¬ ëŒ€í™” ë‚´ìš© ì €ì¥
        st.session_state[f"chat_{chat_id}"] = st.session_state.messages.copy()
        st.session_state.chat_history.append((current_title, chat_id))
        save_chat_history()
    
    # ìƒˆ ëŒ€í™” ì‹œì‘ - ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    st.session_state.memory.clear()
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! Gitbook ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]
    
    # ì¶”ì²œ ì§ˆë¬¸ ì´ˆê¸°í™” - ë²¡í„° DB ê¸°ë°˜ ê³ ê¸‰ ì¶”ì²œ ì§ˆë¬¸
    try:
        # ë²¡í„° DB ê¸°ë°˜ ê³ ê¸‰ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì‹œë„
        st.session_state.suggested_questions = generate_advanced_initial_questions(vector_store, embeddings, supabase_client, qa_llm, num_questions=4)
    except Exception as e:
        print(f"ìƒˆ ëŒ€í™” ì‹œì‘ ì‹œ ê³ ê¸‰ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        try:
            # ëŒ€ì²´ ë°©ì‹ ì‹œë„
            st.session_state.suggested_questions = generate_alternative_questions(supabase_client, qa_llm, num_questions=4)
        except Exception as backup_e:
            print(f"ìƒˆ ëŒ€í™” ì‹œì‘ ì‹œ ëŒ€ì²´ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±ë„ ì‹¤íŒ¨: {backup_e}")
            # ë§ˆì§€ë§‰ ëŒ€ì•ˆìœ¼ë¡œ ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©
            st.session_state.suggested_questions = get_default_questions(4)
    
    st.rerun()

st.sidebar.markdown("---")
st.sidebar.info(
    "ì´ ì±—ë´‡ì€ FETA Gitbook ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."
)

# ì´ì „ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì¶”ì²œ ì§ˆë¬¸ í‘œì‹œ (ì²« ë©”ì‹œì§€ ë˜ëŠ” ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ assistantì¸ ê²½ìš°)
if len(st.session_state.messages) == 1 or st.session_state.messages[-1]["role"] == "assistant":
    # ì¶”ì²œ ì§ˆë¬¸ ì»¨í…Œì´ë„ˆ
    with st.container():
        st.write("#### ì¶”ì²œ ì§ˆë¬¸:")
        cols = st.columns(len(st.session_state.suggested_questions))
        
        for i, question in enumerate(st.session_state.suggested_questions):
            if cols[i].button(question, key=f"suggested_{i}", use_container_width=True):
                handle_suggested_question(question)

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response_content = ""
        
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... ğŸ¤”"):
            try:
                # Langchain QA ì‹¤í–‰ (ConversationalRetrievalChain)
                response = qa_chain.invoke({"question": prompt})
                
                # ì‘ë‹µ ì¶”ì¶œ
                answer = response.get("answer", "")
                source_documents = response.get("source_documents", [])
                    
                if not answer:
                    answer = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ëª…í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

                full_response_content = answer

                if source_documents:
                    full_response_content += "\n\n---\n**ì°¸ê³  ë¬¸ì„œ:**\n"
                    # ì¤‘ë³µëœ source URLì„ ì œê±°í•˜ê¸° ìœ„í•œ set
                    unique_sources = set()
                    for doc in source_documents:
                        source_url = doc.metadata.get('source', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')
                        if source_url not in unique_sources and source_url != 'ì¶œì²˜ ì •ë³´ ì—†ìŒ':
                            # URLì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì œëª©ì²˜ëŸ¼ ì‚¬ìš©
                            link_title = source_url.split('/')[-1] or source_url.split('/')[-2] or "ë¬¸ì„œ"
                            link_title = link_title.replace('-', ' ').title() # ê°€ë…ì„± í–¥ìƒ
                            full_response_content += f"- [{link_title}]({source_url})\n"
                            unique_sources.add(source_url)
                
                message_placeholder.markdown(full_response_content)
                
                # ë§¥ë½ì— ë§ëŠ” ìƒˆë¡œìš´ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±
                context_questions = generate_context_questions(answer, qa_llm)
                if context_questions:
                    st.session_state.suggested_questions = context_questions
                else:
                    # ìƒˆë¡œìš´ ê¸°ë³¸ ì§ˆë¬¸ í‘œì‹œ
                    st.session_state.suggested_questions = random.sample(
                        DEFAULT_SUGGESTED_QUESTIONS, 
                        min(3, len(DEFAULT_SUGGESTED_QUESTIONS))
                    )

            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                full_response_content = "ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                message_placeholder.markdown(full_response_content)
                
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì¶”ì²œ ì§ˆë¬¸ í‘œì‹œ
                st.session_state.suggested_questions = random.sample(
                    DEFAULT_SUGGESTED_QUESTIONS, 
                    min(3, len(DEFAULT_SUGGESTED_QUESTIONS))
                )

        st.session_state.messages.append({"role": "assistant", "content": full_response_content})
        
        # ëŒ€í™” ìë™ ì €ì¥
        # í˜„ì¬ ëŒ€í™” ì´ë¦„ ì €ì¥
        if len(st.session_state.messages) == 3:  # ì²« ë²ˆì§¸ ì§ˆë¬¸ í›„ ì œëª© ìƒì„±
            first_user_msg = prompt
            chat_title = first_user_msg[:15] + ("..." if len(first_user_msg) > 15 else "")
            st.session_state["current_time_str"] = chat_title

# ì±„íŒ… ê¸°ë¡ ì§€ìš°ê¸° ë²„íŠ¼
st.sidebar.markdown("---")
all_cols = st.sidebar.columns([1, 1])

if all_cols[0].button("ëª¨ë“  ëŒ€í™” ì§€ìš°ê¸°", use_container_width=True):
    # ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    st.session_state.memory.clear()
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    st.session_state.chat_history = []
    # í˜„ì¬ ëŒ€í™” ì´ˆê¸°í™”
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! Gitbook ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]
    # ì €ì¥ëœ ëª¨ë“  ëŒ€í™” ì‚­ì œ
    for key in list(st.session_state.keys()):
        if key.startswith("chat_"):
            del st.session_state[key]
    # íŒŒì¼ ì‚­ì œ (ì„ íƒ ì‚¬í•­)
    if os.path.exists(CHAT_HISTORY_FILE):
        try:
            os.remove(CHAT_HISTORY_FILE)
        except:
            pass
    # ì¶”ì²œ ì§ˆë¬¸ ì´ˆê¸°í™” - ë²¡í„° DB ê¸°ë°˜ ê³ ê¸‰ ì¶”ì²œ ì§ˆë¬¸
    try:
        # ë²¡í„° DB ê¸°ë°˜ ê³ ê¸‰ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì‹œë„
        st.session_state.suggested_questions = generate_advanced_initial_questions(vector_store, embeddings, supabase_client, qa_llm, num_questions=4)
    except Exception as e:
        print(f"ëŒ€í™” ì§€ìš°ê¸° ì‹œ ê³ ê¸‰ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        try:
            # ëŒ€ì²´ ë°©ì‹ ì‹œë„
            st.session_state.suggested_questions = generate_alternative_questions(supabase_client, qa_llm, num_questions=4)
        except Exception as backup_e:
            print(f"ëŒ€í™” ì§€ìš°ê¸° ì‹œ ëŒ€ì²´ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±ë„ ì‹¤íŒ¨: {backup_e}")
            # ë§ˆì§€ë§‰ ëŒ€ì•ˆìœ¼ë¡œ ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©
            st.session_state.suggested_questions = get_default_questions(4)
            
    st.rerun()

# í˜„ì¬ ëŒ€í™” ì €ì¥ ë²„íŠ¼
if all_cols[1].button("ëŒ€í™” ì €ì¥í•˜ê¸°", use_container_width=True):
    # ì§ì ‘ í˜„ì¬ ëŒ€í™” ì €ì¥
    if "messages" in st.session_state and len(st.session_state.messages) > 1:
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ID ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        chat_id = f"chat_{timestamp}"
        
        # ëŒ€í™” ì œëª© ìƒì„±
        current_title = st.session_state.get("current_time_str", "ìƒˆ ëŒ€í™”")
        if not current_title or current_title == "ìƒˆ ëŒ€í™”":
            current_title = f"ëŒ€í™” {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}"
        
        # í˜„ì¬ ëŒ€í™” ë‚´ìš© ì €ì¥
        st.session_state[f"chat_{chat_id}"] = st.session_state.messages.copy()
        st.session_state.chat_history.append((current_title, chat_id))
        save_chat_history()
        st.success("ëŒ€í™”ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        st.warning("ì €ì¥í•  ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")