import streamlit as st

# ìŠ¤íŠ¸ë¦¼ë¦¿ í˜ì´ì§€ êµ¬ì„± ì„¤ì • (ë°˜ë“œì‹œ ë‹¤ë¥¸ st ëª…ë ¹ ì „ì— í˜¸ì¶œí•´ì•¼ í•¨)
st.set_page_config(page_title="Gitbook Q&A Chatbot", layout="wide", initial_sidebar_state="expanded")

import os
import json
import datetime
import random
from dotenv import load_dotenv

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores.supabase import SupabaseVectorStore
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from supabase.client import Client, create_client

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_ANON_KEY = os.getenv("SUPABASE_ANON_KEY")
TARGET_GITBOOK_NAME = os.getenv("TARGET_GITBOOK_NAME", "í•´ë‹¹ Gitbook")
CHAT_HISTORY_FILE = os.getenv("CHAT_HISTORY_FILE", "chat_history.json")

# ì¶”ì²œ ì§ˆë¬¸ ëª©ë¡ - ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ì— ë§ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•„ìš”
DEFAULT_SUGGESTED_QUESTIONS = [
    "Gitbookì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
    "FETA í”„ë¡œì íŠ¸ì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "API ë¬¸ì„œëŠ” ì–´ë””ì„œ í™•ì¸í•  ìˆ˜ ìˆë‚˜ìš”?",
    "ê°œë°œì ê°€ì´ë“œëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
    "ì„¤ì¹˜ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
    "ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€"
]

# ë¬¸ë§¥ë³„ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜
def generate_context_questions(last_answer, llm):
    try:
        if not last_answer or len(last_answer) < 20:
            return []
        
        prompt = f"""
        ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤:
        {last_answer[:500]}...

        ìœ„ ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ë‹¤ìŒìœ¼ë¡œ ë¬¼ì–´ë³¼ ë§Œí•œ ê´€ë ¨ ì§ˆë¬¸ 3ê°€ì§€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        ì§§ê³  ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. 
        JSON í˜•ì‹ ì—†ì´ ì§ˆë¬¸ë§Œ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”.
        """
        
        # LLMìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
        response = llm.invoke(prompt)
        questions = response.content.strip().split('\n')
        
        # ë¹ˆ ì¤„ ì œê±°í•˜ê³  ì•ë’¤ ê³µë°± ì œê±°
        questions = [q.strip() for q in questions if q.strip()]
        
        # ìµœëŒ€ 3ê°œ ë°˜í™˜
        return questions[:3]
    except Exception as e:
        print(f"ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return []

# ì±„íŒ… ë‚´ì—­ ì €ì¥ í•¨ìˆ˜
def save_chat_history():
    chat_data = {
        "chat_history": st.session_state.chat_history,
    }
    
    # ê° ëŒ€í™” ë‚´ì—­ë„ ì €ì¥
    for chat_name, chat_id in st.session_state.chat_history:
        chat_key = f"chat_{chat_id}"
        if chat_key in st.session_state:
            chat_data[chat_key] = st.session_state[chat_key]
    
    try:
        with open(CHAT_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(chat_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.warning(f"ì±„íŒ… ë‚´ì—­ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì±„íŒ… ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_chat_history():
    try:
        if os.path.exists(CHAT_HISTORY_FILE):
            with open(CHAT_HISTORY_FILE, 'r', encoding='utf-8') as f:
                chat_data = json.load(f)
                
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
            if "chat_history" in chat_data:
                st.session_state.chat_history = chat_data["chat_history"]
            
            # ê° ëŒ€í™” ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸°
            for key, value in chat_data.items():
                if key.startswith("chat_") and key != "chat_history":
                    st.session_state[key] = value
    except Exception as e:
        st.warning(f"ì±„íŒ… ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì´ˆê¸°í™”
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []

# í™˜ê²½ ë³€ìˆ˜ ìœ íš¨ì„± ê²€ì‚¬
if not OPENAI_API_KEY or not SUPABASE_URL or not SUPABASE_ANON_KEY:
    missing_vars = []
    if not OPENAI_API_KEY:
        missing_vars.append("OPENAI_API_KEY")
    if not SUPABASE_URL:
        missing_vars.append("SUPABASE_URL")
    if not SUPABASE_ANON_KEY:
        missing_vars.append("SUPABASE_ANON_KEY")
    
    st.error(f"í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing_vars)}")
    st.info("create_env.py ìŠ¤í¬ë¦½íŠ¸ë¡œ ìƒì„±ëœ .env íŒŒì¼ì„ í¸ì§‘í•˜ì—¬ í•„ìš”í•œ ê°’ì„ ì±„ì›Œì£¼ì„¸ìš”.")
    st.stop()

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìºì‹±)
@st.cache_resource
def init_supabase_client():
    try:
        return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)
    except Exception as e:
        st.error(f"Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

# ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìºì‹±)
@st.cache_resource
def init_chat_model():
    return ChatOpenAI(
        temperature=0.1, 
        model_name='gpt-3.5-turbo', 
        openai_api_key=OPENAI_API_KEY
    )

supabase_client = init_supabase_client()
if not supabase_client:
    st.stop()

# LLM ëª¨ë¸ ì´ˆê¸°í™”
llm = init_chat_model()

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ)
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
    load_chat_history()

# ì¶”ì²œ ì§ˆë¬¸ ì´ˆê¸°í™”
if "suggested_questions" not in st.session_state:
    st.session_state.suggested_questions = random.sample(DEFAULT_SUGGESTED_QUESTIONS, min(4, len(DEFAULT_SUGGESTED_QUESTIONS)))

# ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ì„¸ì…˜ ìƒíƒœ ì‚¬ìš©)
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )

# Langchain êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìºì‹±)
@st.cache_resource
def init_langchain_components(_supabase_client, _memory): 
    try:
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        
        # Supabase Vector Store ì´ˆê¸°í™”
        vector_store = SupabaseVectorStore(
            client=_supabase_client,
            embedding=embeddings,
            table_name="documents",
            query_name="match_documents"
        )
        
        llm = ChatOpenAI(
            temperature=0.1, 
            model_name='gpt-3.5-turbo', 
            openai_api_key=OPENAI_API_KEY
        )
        
        # ê²€ìƒ‰ê¸° ì„¤ì •
        retriever = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={
                'k': 5, 
                'score_threshold': 0.5, 
                'filter': {} 
            } 
        )
        
        # ConversationalRetrievalChain ì‚¬ìš© (ëŒ€í™” ê¸°ì–µ ê¸°ëŠ¥ í¬í•¨)
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=_memory,
            return_source_documents=True,
            return_generated_question=True,
        )
            
        return qa_chain
    except Exception as e:
        st.error(f"Langchain êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

qa_chain = init_langchain_components(supabase_client, st.session_state.memory)
if not qa_chain:
    st.stop()

# ì¶”ì²œ ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜
def handle_suggested_question(question):
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ì±„íŒ…ì°½ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": question})
    
    # ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response_content = ""
        
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... ğŸ¤”"):
            try:
                # Langchain QA ì‹¤í–‰
                response = qa_chain.invoke({"question": question})
                
                # ì‘ë‹µ ì¶”ì¶œ
                answer = response.get("answer", "")
                source_documents = response.get("source_documents", [])
                    
                if not answer:
                    answer = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ëª…í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

                full_response_content = answer

                if source_documents:
                    full_response_content += "\n\n---\n**ì°¸ê³  ë¬¸ì„œ:**\n"
                    # ì¤‘ë³µëœ source URLì„ ì œê±°í•˜ê¸° ìœ„í•œ set
                    unique_sources = set()
                    for doc in source_documents:
                        source_url = doc.metadata.get('source', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')
                        if source_url not in unique_sources and source_url != 'ì¶œì²˜ ì •ë³´ ì—†ìŒ':
                            # URLì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì œëª©ì²˜ëŸ¼ ì‚¬ìš©
                            link_title = source_url.split('/')[-1] or source_url.split('/')[-2] or "ë¬¸ì„œ"
                            link_title = link_title.replace('-', ' ').title() # ê°€ë…ì„± í–¥ìƒ
                            full_response_content += f"- [{link_title}]({source_url})\n"
                            unique_sources.add(source_url)
                
                message_placeholder.markdown(full_response_content)
                
                # ë§¥ë½ì— ë§ëŠ” ìƒˆë¡œìš´ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±
                context_questions = generate_context_questions(answer, llm)
                if context_questions:
                    st.session_state.suggested_questions = context_questions
                else:
                    # ìƒˆë¡œìš´ ê¸°ë³¸ ì§ˆë¬¸ í‘œì‹œ
                    st.session_state.suggested_questions = random.sample(
                        DEFAULT_SUGGESTED_QUESTIONS, 
                        min(3, len(DEFAULT_SUGGESTED_QUESTIONS))
                    )

            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                full_response_content = "ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                message_placeholder.markdown(full_response_content)
                
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì¶”ì²œ ì§ˆë¬¸ í‘œì‹œ
                st.session_state.suggested_questions = random.sample(
                    DEFAULT_SUGGESTED_QUESTIONS, 
                    min(3, len(DEFAULT_SUGGESTED_QUESTIONS))
                )

        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": full_response_content})
        
        # ëŒ€í™” ìë™ ì €ì¥
        # í˜„ì¬ ëŒ€í™” ì´ë¦„ ì €ì¥
        if len(st.session_state.messages) == 3:  # ì²« ë²ˆì§¸ ì§ˆë¬¸ í›„ ì œëª© ìƒì„±
            first_user_msg = question
            chat_title = first_user_msg[:15] + ("..." if len(first_user_msg) > 15 else "")
            st.session_state["current_time_str"] = chat_title
    
    # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
    st.rerun()

# --- Streamlit UI ---
st.title("ğŸ“š Gitbook Q&A Chatbot")
st.caption(f"'{TARGET_GITBOOK_NAME}' ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ ì„œë¹„ìŠ¤") 

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("ì±—ë´‡ ì„¤ì •")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì„¹ì…˜ ì¶”ê°€
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬")

# ì €ì¥ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
history_cols = st.sidebar.columns([4, 1])
for i, (chat_name, chat_id) in enumerate(st.session_state.chat_history):
    # ëŒ€í™” ì„ íƒ
    if history_cols[0].button(f"{chat_name}", key=f"history_{i}", use_container_width=True):
        # ì„ íƒí•œ ëŒ€í™” ë‚´ìš© ë¶ˆëŸ¬ì˜¤ê¸°
        st.session_state.messages = st.session_state[f"chat_{chat_id}"].copy()
        # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (í•´ë‹¹ ëŒ€í™”ì— ë§ê²Œ)
        st.session_state.memory.clear()
        # ë©”ëª¨ë¦¬ ì¬êµ¬ì„± (ëŒ€í™” ë‚´ìš© ê¸°ë°˜)
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                user_msg = msg["content"]
            elif msg["role"] == "assistant" and user_msg:
                st.session_state.memory.chat_memory.add_user_message(user_msg)
                st.session_state.memory.chat_memory.add_ai_message(msg["content"])
                user_msg = None
        st.rerun()
    
    # ëŒ€í™” ì‚­ì œ ë²„íŠ¼
    if history_cols[1].button("ğŸ—‘ï¸", key=f"delete_{i}", help="ì´ ëŒ€í™” ì‚­ì œí•˜ê¸°"):
        # ëŒ€í™” ì‚­ì œ í™•ì¸
        chat_key = f"chat_{chat_id}"
        if chat_key in st.session_state:
            del st.session_state[chat_key]
        
        # íˆìŠ¤í† ë¦¬ì—ì„œ ì œê±°
        st.session_state.chat_history.pop(i)
        save_chat_history()
        st.rerun()

# ìƒˆ ëŒ€í™” ì‹œì‘ ë²„íŠ¼
if st.sidebar.button("â• ìƒˆ ëŒ€í™” ì‹œì‘", use_container_width=True):
    # í˜„ì¬ ëŒ€í™”ê°€ ìˆìœ¼ë©´ ì €ì¥
    if "messages" in st.session_state and len(st.session_state.messages) > 1:
        # ìƒˆ ëŒ€í™” ID ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
        timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        chat_id = f"chat_{timestamp}"
        
        # ëŒ€í™” ì œëª© ìƒì„±
        current_title = st.session_state.get("current_time_str", "ìƒˆ ëŒ€í™”")
        
        # í˜„ì¬ ëŒ€í™” ë‚´ìš© ì €ì¥
        st.session_state[f"chat_{chat_id}"] = st.session_state.messages.copy()
        st.session_state.chat_history.append((current_title, chat_id))
        save_chat_history()
    
    # ìƒˆ ëŒ€í™” ì‹œì‘ - ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    st.session_state.memory.clear()
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! Gitbook ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]
    
    # ì¶”ì²œ ì§ˆë¬¸ ì´ˆê¸°í™”
    st.session_state.suggested_questions = random.sample(
        DEFAULT_SUGGESTED_QUESTIONS, 
        min(4, len(DEFAULT_SUGGESTED_QUESTIONS))
    )
    
    st.rerun()

st.sidebar.markdown("---")
st.sidebar.info(
    "ì´ ì±—ë´‡ì€ FETA Gitbook ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."
)

# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! Gitbook ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]

# ì´ì „ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì¶”ì²œ ì§ˆë¬¸ í‘œì‹œ (ì²« ë©”ì‹œì§€ ë˜ëŠ” ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ assistantì¸ ê²½ìš°)
if len(st.session_state.messages) == 1 or st.session_state.messages[-1]["role"] == "assistant":
    # ì¶”ì²œ ì§ˆë¬¸ ì»¨í…Œì´ë„ˆ
    with st.container():
        st.write("#### ì¶”ì²œ ì§ˆë¬¸:")
        cols = st.columns(len(st.session_state.suggested_questions))
        
        for i, question in enumerate(st.session_state.suggested_questions):
            if cols[i].button(question, key=f"suggested_{i}", use_container_width=True):
                handle_suggested_question(question)

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response_content = ""
        
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... ğŸ¤”"):
            try:
                # Langchain QA ì‹¤í–‰ (ConversationalRetrievalChain)
                response = qa_chain.invoke({"question": prompt})
                
                # ì‘ë‹µ ì¶”ì¶œ
                answer = response.get("answer", "")
                source_documents = response.get("source_documents", [])
                    
                if not answer:
                    answer = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ëª…í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

                full_response_content = answer

                if source_documents:
                    full_response_content += "\n\n---\n**ì°¸ê³  ë¬¸ì„œ:**\n"
                    # ì¤‘ë³µëœ source URLì„ ì œê±°í•˜ê¸° ìœ„í•œ set
                    unique_sources = set()
                    for doc in source_documents:
                        source_url = doc.metadata.get('source', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')
                        if source_url not in unique_sources and source_url != 'ì¶œì²˜ ì •ë³´ ì—†ìŒ':
                            # URLì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì œëª©ì²˜ëŸ¼ ì‚¬ìš©
                            link_title = source_url.split('/')[-1] or source_url.split('/')[-2] or "ë¬¸ì„œ"
                            link_title = link_title.replace('-', ' ').title() # ê°€ë…ì„± í–¥ìƒ
                            full_response_content += f"- [{link_title}]({source_url})\n"
                            unique_sources.add(source_url)
                
                message_placeholder.markdown(full_response_content)
                
                # ë§¥ë½ì— ë§ëŠ” ìƒˆë¡œìš´ ì¶”ì²œ ì§ˆë¬¸ ìƒì„±
                context_questions = generate_context_questions(answer, llm)
                if context_questions:
                    st.session_state.suggested_questions = context_questions
                else:
                    # ìƒˆë¡œìš´ ê¸°ë³¸ ì§ˆë¬¸ í‘œì‹œ
                    st.session_state.suggested_questions = random.sample(
                        DEFAULT_SUGGESTED_QUESTIONS, 
                        min(3, len(DEFAULT_SUGGESTED_QUESTIONS))
                    )

            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                full_response_content = "ì£„ì†¡í•©ë‹ˆë‹¤, í˜„ì¬ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                message_placeholder.markdown(full_response_content)
                
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì¶”ì²œ ì§ˆë¬¸ í‘œì‹œ
                st.session_state.suggested_questions = random.sample(
                    DEFAULT_SUGGESTED_QUESTIONS, 
                    min(3, len(DEFAULT_SUGGESTED_QUESTIONS))
                )

        st.session_state.messages.append({"role": "assistant", "content": full_response_content})
        
        # ëŒ€í™” ìë™ ì €ì¥
        # í˜„ì¬ ëŒ€í™” ì´ë¦„ ì €ì¥
        if len(st.session_state.messages) == 3:  # ì²« ë²ˆì§¸ ì§ˆë¬¸ í›„ ì œëª© ìƒì„±
            first_user_msg = prompt
            chat_title = first_user_msg[:15] + ("..." if len(first_user_msg) > 15 else "")
            st.session_state["current_time_str"] = chat_title

# ì±„íŒ… ê¸°ë¡ ì§€ìš°ê¸° ë²„íŠ¼
st.sidebar.markdown("---")
all_cols = st.sidebar.columns([1, 1])

if all_cols[0].button("ëª¨ë“  ëŒ€í™” ì§€ìš°ê¸°", use_container_width=True):
    # ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    st.session_state.memory.clear()
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    st.session_state.chat_history = []
    # í˜„ì¬ ëŒ€í™” ì´ˆê¸°í™”
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! Gitbook ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."}]
    # ì €ì¥ëœ ëª¨ë“  ëŒ€í™” ì‚­ì œ
    for key in list(st.session_state.keys()):
        if key.startswith("chat_"):
            del st.session_state[key]
    # íŒŒì¼ ì‚­ì œ (ì„ íƒ ì‚¬í•­)
    if os.path.exists(CHAT_HISTORY_FILE):
        try:
            os.remove(CHAT_HISTORY_FILE)
        except:
            pass
    # ì¶”ì²œ ì§ˆë¬¸ ì´ˆê¸°í™”
    st.session_state.suggested_questions = random.sample(
        DEFAULT_SUGGESTED_QUESTIONS, 
        min(4, len(DEFAULT_SUGGESTED_QUESTIONS))
    )
    st.rerun()

# í˜„ì¬ ëŒ€í™” ì €ì¥ ë²„íŠ¼
if all_cols[1].button("ëŒ€í™” ì €ì¥í•˜ê¸°", use_container_width=True):
    # ì§ì ‘ í˜„ì¬ ëŒ€í™” ì €ì¥
    if "messages" in st.session_state and len(st.session_state.messages) > 1:
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ID ìƒì„±
        timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
        chat_id = f"chat_{timestamp}"
        
        # ëŒ€í™” ì œëª© ìƒì„±
        current_title = st.session_state.get("current_time_str", "ìƒˆ ëŒ€í™”")
        if not current_title or current_title == "ìƒˆ ëŒ€í™”":
            current_title = f"ëŒ€í™” {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}"
        
        # í˜„ì¬ ëŒ€í™” ë‚´ìš© ì €ì¥
        st.session_state[f"chat_{chat_id}"] = st.session_state.messages.copy()
        st.session_state.chat_history.append((current_title, chat_id))
        save_chat_history()
        st.success("ëŒ€í™”ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        st.warning("ì €ì¥í•  ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")